{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyRgaHF5Vqh3",
        "outputId": "a8a5c842-e912-4050-f6da-e3d5052e1ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDj1sTUobM5O",
        "outputId": "8ad1d7f3-3f85-47dc-e349-bbd4aa275efb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from pyspark.sql.functions import explode, col, to_timestamp, substring\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, BooleanType, ArrayType"
      ],
      "metadata": {
        "id": "Hl7UFvaUbSZk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "storage_account_name = userdata.get(\"AZURE_ACCOUNT_NAME\")\n",
        "storage_account_key = userdata.get(\"AZURE_STORAGE_KEY\")\n",
        "\n",
        "storage_container_name = \"kaggle-datasets\"\n",
        "parquet_blob_name = \"github-dataset-full.parquet\"\n",
        "\n"
      ],
      "metadata": {
        "id": "tXNMmn0rbX1e"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read Parquet from Azure Blob Storage\") \\\n",
        "    .config(f\"spark.hadoop.fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key) \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.2,com.microsoft.azure:azure-storage:8.6.6\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Remove garbage error texts\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")"
      ],
      "metadata": {
        "id": "P9gAW7QBeyiX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "outputId": "8153f20f-dfa7-49c4-e4e9-85072b06b90a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "The value of property spark.hadoop.fs.azure.account.key.None.blob.core.windows.net must not be null\n\nJVM stacktrace:\njava.lang.IllegalArgumentException: The value of property spark.hadoop.fs.azure.account.key.None.blob.core.windows.net must not be null\n\tat org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:219)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1403)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1384)\n\tat org.apache.spark.sql.internal.SharedState.$anonfun$x$1$2(SharedState.scala:77)\n\tat scala.collection.immutable.Map$Map4.foreach(Map.scala:493)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:69)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat org.apache.spark.sql.SparkSession$.conf$lzycompute$1(SparkSession.scala:1213)\n\tat org.apache.spark.sql.SparkSession$.conf$1(SparkSession.scala:1213)\n\tat org.apache.spark.sql.SparkSession$.applyModifiableSettings(SparkSession.scala:1216)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e15a7fd5c1b4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.jars.packages\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.hadoop:hadoop-azure:3.3.2,com.microsoft.azure:azure-storage:8.6.6\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.driver.memory\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"8g\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Remove garbage error texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    502\u001b[0m                     getattr(\n\u001b[1;32m    503\u001b[0m                         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SparkSession$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MODULE$\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m                     ).applyModifiableSettings(session._jsparkSession, self._options)\n\u001b[0m\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: The value of property spark.hadoop.fs.azure.account.key.None.blob.core.windows.net must not be null\n\nJVM stacktrace:\njava.lang.IllegalArgumentException: The value of property spark.hadoop.fs.azure.account.key.None.blob.core.windows.net must not be null\n\tat org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:219)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1403)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1384)\n\tat org.apache.spark.sql.internal.SharedState.$anonfun$x$1$2(SharedState.scala:77)\n\tat scala.collection.immutable.Map$Map4.foreach(Map.scala:493)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:69)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat org.apache.spark.sql.SparkSession$.conf$lzycompute$1(SparkSession.scala:1213)\n\tat org.apache.spark.sql.SparkSession$.conf$1(SparkSession.scala:1213)\n\tat org.apache.spark.sql.SparkSession$.applyModifiableSettings(SparkSession.scala:1216)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\n",
        "    f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\",storage_account_key)\n",
        "\n",
        "\n",
        "# Set path to parquet file for access\n",
        "parquet_path = f\"wasbs://{storage_container_name}@{storage_account_name}.blob.core.windows.net/{parquet_blob_name}\"\n"
      ],
      "metadata": {
        "id": "PvwTsdy9eK4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_df = spark.read.parquet(\n",
        "    \"wasbs://kaggle-datasets@matthewleffler1.blob.core.windows.net/clean_data/non_list_data\"\n",
        ")\n",
        "\n",
        "repo_list_df = spark.read.parquet(\n",
        "    \"wasbs://kaggle-datasets@matthewleffler1.blob.core.windows.net/clean_data/repo_list_data\"\n",
        ")"
      ],
      "metadata": {
        "id": "ERF3CHK_ecRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, coalesce, lit\n",
        "\n",
        "\n",
        "# SQL: License Trend by Language\n",
        "license_trend_df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        license,\n",
        "        language,\n",
        "        COUNT(*) AS repo_count,\n",
        "        SUM(CAST(stargazers_count AS INT)) AS total_stars\n",
        "    FROM repo_list\n",
        "    WHERE license IS NOT NULL AND license != ''\n",
        "    GROUP BY license, language\n",
        "    ORDER BY repo_count DESC\n",
        "\"\"\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rEJaaWrkmvOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "license_trend_df.show(20, truncate=False)"
      ],
      "metadata": {
        "id": "jsAUjgPCnvDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "license_trend_df.orderBy(\"total_stars\", ascending=False).show(20, truncate=False)"
      ],
      "metadata": {
        "id": "FElFi27Un2i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Spark DataFrame to Pandas for plotting\n",
        "license_trend_pd = license_trend_df.toPandas()"
      ],
      "metadata": {
        "id": "UqeU255un2fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Top 10 licenses by repo count\n",
        "top_license_trend = license_trend_pd.sort_values(by=\"repo_count\", ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=\"repo_count\", y=\"license\", data=top_license_trend, palette=\"viridis\")\n",
        "plt.title(\"Top 10 Open Source Licenses by Repository Count\")\n",
        "plt.xlabel(\"Repository Count\")\n",
        "plt.ylabel(\"License\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Top 10 licenses by total stars\n",
        "top_stars_trend = license_trend_pd.sort_values(by=\"total_stars\", ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=\"total_stars\", y=\"license\", data=top_stars_trend, palette=\"magma\")\n",
        "plt.title(\"Top 10 Open Source Licenses by Total Stargazers\")\n",
        "plt.xlabel(\"Total Stars\")\n",
        "plt.ylabel(\"License\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zm6D9jUVn2Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sfOptions = {\n",
        "    \"sfURL\" : \"https://bfdeidg-mvb00946.snowflakecomputing.com\",\n",
        "    \"sfDatabase\" : \"BIGDATA_GITHUB\",\n",
        "    \"sfSchema\" : \"ANALYTICS\",\n",
        "    \"sfWarehouse\" : \"COMPUTE_WH\",\n",
        "    \"sfRole\": \"ACCOUNTADMIN\",  # or appropriate role\n",
        "    \"sfUser\" : \"bigdata228\",\n",
        "    \"sfPassword\" : \"SJSUbigdata@1234\"\n",
        "}\n",
        "\n",
        "# Write to Snowflake\n",
        "license_trend_df.write \\\n",
        "    .format(\"snowflake\") \\\n",
        "    .options(**sfOptions) \\\n",
        "    .option(\"dbtable\", \"license_trends_summary\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save()\n"
      ],
      "metadata": {
        "id": "s168Fv6bnccR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}