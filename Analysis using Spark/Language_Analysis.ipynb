{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/anaconda3/envs/naturalistvenv/lib/python3.12/site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/anaconda3/envs/naturalistvenv/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/naturalistvenv/lib/python3.12/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install pyspark\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pyspark.sql.functions import explode, col, to_timestamp, substring, lower, trim, countDistinct, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, BooleanType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables for Azure access information\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "storage_account_name = os.getenv(\"AZURE_ACCOUNT_NAME\")\n",
    "storage_account_key = os.getenv(\"AZURE_STORAGE_KEY\")\n",
    "storage_container_name = \"kaggle-datasets\"\n",
    "parquet_blob_name = \"github-dataset-full.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/envs/naturalistvenv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/matthewleffler/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/matthewleffler/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-azure added as a dependency\n",
      "com.microsoft.azure#azure-storage added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-48bb7c42-ff7d-4671-a2ed-b71269612e4c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.2 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in local-m2-cache\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in local-m2-cache\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.microsoft.azure#azure-storage;8.6.6 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.9.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.12 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.4 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.2.4 in central\n",
      "\tfound com.google.guava#guava;24.1.1-jre in central\n",
      "\tfound com.google.code.findbugs#jsr305;1.3.9 in local-m2-cache\n",
      "\tfound org.checkerframework#checker-compat-qual;2.0.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in local-m2-cache\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n",
      ":: resolution report :: resolve 992ms :: artifacts dl 36ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.9.4 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;1.3.9 from local-m2-cache in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.1.3 from central in [default]\n",
      "\tcom.google.guava#guava;24.1.1-jre from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from local-m2-cache in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.2.4 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;8.6.6 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from local-m2-cache in [default]\n",
      "\torg.checkerframework#checker-compat-qual;2.0.0 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.12 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 by [com.microsoft.azure#azure-storage;8.6.6] in [default]\n",
      "\torg.apache.commons#commons-lang3;3.8.1 by [org.apache.commons#commons-lang3;3.4] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   24  |   0   |   0   |   2   ||   22  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-48bb7c42-ff7d-4671-a2ed-b71269612e4c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 22 already retrieved (0kB/22ms)\n",
      "25/05/09 20:53:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/09 20:53:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Creating Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Parquet from Azure Blob Storage\") \\\n",
    "    .config(f\"spark.hadoop.fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.2,com.microsoft.azure:azure-storage:8.6.6\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Remove garbage error texts\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set authentification for Spark to connect to Azure\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\",\n",
    "    storage_account_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Repo List Data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read data to ensure data was properly saved\n",
    "repo_list_df = spark.read.parquet(\n",
    "    \"wasbs://kaggle-datasets@matthewleffler1.blob.core.windows.net/clean_data/repo_list_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_list_df = (\n",
    "    repo_list_df\n",
    "    .withColumn(\n",
    "        \"repo_language\",\n",
    "        when(lower(col(\"repo_language\")) == \"fortran\", \"Fortran\")\n",
    "        .when(lower(col(\"repo_language\")) == \"haxe\", \"Haxe\")\n",
    "        .when(lower(col(\"repo_language\")) == \"ecl\", \"ECL\")\n",
    "        .otherwise(col(\"repo_language\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Abstractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "language_popularity_df = (\n",
    "    repo_list_df\n",
    "    .na.drop(subset=[\"repo_language\"])\n",
    "    .withColumn(\"year_month\", date_format(\"repo_created_at\", \"yyyy-MM\"))\n",
    "    .groupBy(\"year_month\", \"repo_language\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"user_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------+\n",
      "|year_month|   repo_language|user_count|\n",
      "+----------+----------------+----------+\n",
      "|   2013-12|      JavaScript|     17245|\n",
      "|   2015-11|          Python|     20173|\n",
      "|   2014-11|            Ruby|      7054|\n",
      "|   2016-07|           Scala|      1504|\n",
      "|   2017-07|Jupyter Notebook|      7331|\n",
      "|   2017-09|      TypeScript|      8720|\n",
      "|   2018-01|         Gnuplot|        12|\n",
      "|   2015-09|           Scala|      1275|\n",
      "|   2018-03|          Rascal|       108|\n",
      "|   2013-08|           Shell|      1815|\n",
      "|   2017-05|      FreeMarker|        56|\n",
      "|   2011-10|         Haskell|        50|\n",
      "|   2015-04|               D|        41|\n",
      "|   2014-11|        Assembly|       141|\n",
      "|   2014-04|    CoffeeScript|       626|\n",
      "|   2013-04|             CSS|       557|\n",
      "|   2017-12|         Haskell|       669|\n",
      "|   2018-07|          Kotlin|      1478|\n",
      "|   2018-08|      Vim script|        74|\n",
      "|   2014-12|             Lua|       457|\n",
      "+----------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "language_popularity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                         (0 + 8) / 19]\r"
     ]
    }
   ],
   "source": [
    "# # # Write data to Azure container\n",
    "language_popularity_df.write.mode(\"overwrite\").parquet(\n",
    "    \"wasbs://kaggle-datasets@matthewleffler1.blob.core.windows.net/analytics/language_popularity\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data to ensure data was properly saved\n",
    "test_df = spark.read.parquet(\n",
    "    \"wasbs://kaggle-datasets@matthewleffler1.blob.core.windows.net/analytics/language_popularity\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Files to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Replace the placeholders with your actual Snowflake credentials\n",
    "conn = snowflake.connector.connect(\n",
    "    user = os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    password = os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    account= os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    warehouse='COMPUTE_WH',\n",
    "    database='BIGDATA_GITHUB',\n",
    "    schema='ANALYTICS',\n",
    "    role='ACCOUNTADMIN'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "url = 'azure://matthewleffler1.blob.core.windows.net/kaggle-datasets/analytics/language_popularity/'\n",
    "\n",
    "try:\n",
    "    cur.execute(\"BEGIN;\")\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE OR REPLACE STAGE azure_parquet_stage_language_popularity\n",
    "          URL = '{url}'\n",
    "          CREDENTIALS = (\n",
    "            AZURE_SAS_TOKEN = '{os.getenv(\"AZURE_SAS_TOKEN\")}'\n",
    "          )\n",
    "          FILE_FORMAT = (TYPE = PARQUET);\n",
    "        \"\"\")\n",
    "    cur.execute(\"COMMIT;\")\n",
    "    print(f\"Successfullt created stage.\")\n",
    "except Exception as e:\n",
    "    cur.execute(\"ROLLBACK;\")\n",
    "    print(f\"Error creating database object: {e}\")\n",
    "finally:\n",
    "    cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "table_name = \"LANGUAGE_POPULARITY_TABLE\"\n",
    "\n",
    "try:\n",
    "    cur.execute(\"BEGIN;\")\n",
    "    cur.execute(f\"\"\"\n",
    "      CREATE OR REPLACE TABLE {table_name} (\n",
    "          year_month STRING,\n",
    "          repo_language STRING,\n",
    "          user_count BIGINT\n",
    "      );\n",
    "          \"\"\")\n",
    "    cur.execute(\"COMMIT;\")\n",
    "    print(\"Table created successfully.\")\n",
    "except Exception as e:\n",
    "    cur.execute(\"ROLLBACK;\")\n",
    "    print(f\"Error creating database object: {e}\")\n",
    "finally:\n",
    "    cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "\n",
    "try:\n",
    "    cur.execute(\"BEGIN;\")\n",
    "    cur.execute(f\"\"\"\n",
    "      COPY INTO {table_name}\n",
    "      FROM @azure_parquet_stage_language_popularity\n",
    "      FILE_FORMAT = (TYPE = PARQUET)\n",
    "      MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE\n",
    "      ON_ERROR = CONTINUE;\n",
    "          \"\"\")\n",
    "    cur.execute(\"COMMIT;\")\n",
    "    print(f\"Data loaded into {table_name} successfully.\")\n",
    "except Exception as e:\n",
    "    cur.execute(\"ROLLBACK;\")\n",
    "    print(f\"Error loading data: {e}\")\n",
    "finally:\n",
    "    cur.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (naturalistvenv)",
   "language": "python",
   "name": "naturalistvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
